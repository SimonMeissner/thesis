\chapter{Discussion}
$n_view$ no impact on answer time


distraction index transformation and difficulty index transformation surprisingly showed matching results although the latter already
included all three variables.

\section{Finding mathematical models}
Besides the view quality coefficient the aggregated variables that based on our partly nominal-scaled study variables showed almost no correlation with
the answer time. Trying to fit an linear regression model on the aggregated variables also turned out to be not successful. Because of the exploratory nature
of this study and because of the little confidence in building the aggregated variables it was almost expected. As a result we tried transforming various variables
from the mathematical foundation we assembled. The transformation of the view distraction values showed that an exponential influence of the distraction from each view
better describes the measured answer times in comparison to a linear influence. Including this finding in a multiple linear regression model using all predictors
resulted in a still really poorly descriptive model with an adjusted $R^2$ value of $0.14$. Next up we tried transforming the distraction index from \ref{distractionIndexEquation}
directly. We again found that an exponential influence of the distraction index (which describes the total distraction present) better described our recorded
anser time values compared to an linear influence. The model using again all three variables showed an adjusted $R^2$ value of $0.16$ which also meant that the exponentially
transformed distraction index described the answer times slightly better compared to the previous transformation. We tried combining both transformations but the best
multiple linear regression model only reached the same adjusted $R^2$ value of the view distraction transformation. Lastly we aggregated all our aggregated variables
into one called difficulty index (equation \ref{difficultyIndexEquation}) and observed how transformations of this value would perform compared to the other approaches.
The model had and adjusted $R^2$ value of $0.078$ and therefore described the answer times even worse than or initial model with no transformations. This was most likely
to be the case because we reduced the number of coefficient that could correct the error.
We also fitted multiple linear models over the mean (and in our case also median) values of answer times of every unique variable combination because human performance
tends to vary a lot across different participants. The best model scored an adjusted $R^2$ value of $0.18$ which was still a poor descriptive model. Excluding
the number of views variable as it showed no relation to the answer time did only reduce the quality of the model. Concluding we can say that the distraction index
did not describe our measured answer time values well. It seemed that an exponential influence of the distraction performs better compared to an linear influence.
But this must be taken with caution because brute forcing different transformation always come at the cost of fitting the data accuratly by chance 

\section{Quality of view}
Out of all aggregated variables $\frac{1}{q_{view}}$ was the only variable that showed an moderate positive relation to the answer time with a linear correlation coefficient of $0.33$.
Because the levene test of the residual of the anova test using the untransformed answer time values showed uneven variances across groups the anova test was repeated
using log10-transformed answer values. The levene test returned an p-value of $0.17$ which means the null hypothesis of can be accepted an the variances across groups are even.
The result of the anova test between the log10-transformed answer time values and the $\frac{1}{q_{view}}$ showed that there were siginificant differences between
the two groups of view quality because of the p-value beeing < $0.05$. Together with the correlation coefficient we can conclude that the mean log10-transformed answer times
grow if the quality of the views fall. This does not only give evidence to our assumed mathematical correlation captured in equation \ref{qualityViewEquation} it also backs up
the observations about view quality regarding the information conveyed in the paper from Lohse et al. from 1994 \citep{Lohse.1994}.

\section{Analysis of study variables}
The variance analysis of the raw study variables and the answer time delivered different results. We setted the confidence level for all tests to $0.05$. The anova tests
about the answer time revealed that we can reject the null hypothesis (answer time means are equal across groups) about two of the four variables.
We used levene tests to check for homoscedasticity in all variables. The null hypothesis of the levene tests states that the variances across groups stay equal.
The levene test between the residuals of the anova test between the number of comparison targets and the answer time showed an p-value of $0.029$ displays an significant result
because of the significance level of $0.05$. That means we have to reject the null hypothesis and the condition of homoscedasticity for the anova test is not met.
A log10-transformation of the the answer time resulted in a p-value of $0.66$ in the levene test which confirmed homoscedasticity after the transformation.
The anova test between the log10-transformed answer time values and the number of comparison targets delivered an p-value of $0.00012$ which meant we could reject the
null hypothesis and we have significantly differences between the groups of number of comparison targets regarding the mean log10-transformed answer time values.
Because the number of comparison targets only had two groups (2 targets or 3 targets) we can also conclude that the means of the answer time between comparison
questions addressing 2 comparison targets and comparison questions addressing 3 comparison targets significantly differ. Looking at figure \ref{boxplotsStudyVariablesAnswerTimefigure}
we can graphically derive that the mean of answer time increased with the number of comparison targets. This observation comfirmes the statement of Gleicher that
the comparison task grows more difficult the more targets are compared simultaneously \citep*{Gleicher.2018}. 
The levene test between the residuals of the anova test with untransformed values between the question type and the answer time also delivered an p-value < $0.05$.
We also used a log10-transformation on the answer time to introduce homoscedasticity. The anova test with the question type also showed an p-value of
< $0.05$ ($1.68 \times 10^{-7}$) which means that there are also statistical significant mean differences across the 4 question types in terms of the
log10-transformed answer time. Because we did not include a post-hoc analysis at this point we can only speculate about the types that differ from another. Looking
again at figure \ref{boxplotsStudyVariablesAnswerTimefigure} we can visually see an increase in mean answer time for type4 in comparison to the remaining question types.
This coincides with the often reported high difficulty of the question of type4 using 3 comparison targets from the participants.
The anova test on the number of views and the interaction technique did not show any significant mean answer time differences across groups as their p-values were way above
the confidence level of $0.05$. Regarding the second research question we could not detect any significant performance differences in terms of time needed to answer comparison questions
across the different interaction techniques. Visually small differences between the the performance of the interaction techniques regarding answer time are visible
(figure \ref{boxplotsStudyVariablesAnswerTimefigure}), but nothing that could be interpreted as beeing statistically significant in any way.

The influence of the study variables on the answer accuracy was measured using the chi squared test and the fisher's exact test. Regarding the
number of view, the number of comparison targets and the interaction technique the tests always returned an p-value way greater than $0.05$ which means
that we have to reject the null hypothesis about there beeing significant differences on the answer accuracy across groups. Interestingly the
chi squared test about the question type returned an p-value of $0.023$ which is < $0.05$. Because one of the precondition of the chi squared test
required the expected cell frequencies of the contengency table to be not smaller than 5 we validated the result using the fisher's exact test which also
returned an p-value < $0.05$ ($0.039$) and therefore indicates significant differences across groups regarding the answer accuracy. Meaning there is a significant
difference between correctly and incorrectly answered questions between at least two groups of question types.


\section{Limitations}
The approach of aggregating the variables into continuous variables not only requires careful consideration of the underlying theoretical framework
but it also requires previous multivariate analysis to reduce exclude effects of multicorrinality \citep*{MichelaNardo.2005}. We did not check for multicorrinality
at all. The linear regression analysis showed mixed results.
It can be argued that the web-prototype disregarded some of the proposed guidelines from Baldonado on when to use MCV. The rule of diversity
suggests the existence of complex data with multiple attributes or models \citep*{WangBaldonado.2000}. Especially on the questions that did not asked for differences among
the comparison targets (type1 and type3) the additional views only presented more distraction from the target values. In these questions the additional views did not
help the participant in terms of user performance it only increased cognitive demand. This conflicts with our aggregated variable from \ref{difficultyIndexEquation} where
the difficulty index was modelled to be inverse dependent to the number of views. We can support this consideration with the missing observed influence of the number of views
on the answer time. 
Because the variance analysis showed significance of some variables (question type, number of comparison targets)
we can assume that the distraction index did not accurately maps the influence of these variables. 

The whole study suffered from a way to small sample size. Particularly the analysis on the mean values of the observations is expected to provide more meaningful
results because averaging over 2 observations per variable combination does not really address the issue of high variance of performance across the participants.
The participants were also quite homogeneous in terms of their age (all 20-29) and experience with geo-dashboards. Our results are therefore not really representative
for the general population. The extent to which the number of comparison targets and the question type influence the answer time has to be further investigated.


\section{Further research}
Trinchera et al. differenciate between three different approaches when trying to formulate aggregated variables \citep*{Trinchera.2008}. We should have
investigated more time in choosing the right approach to formulate our aggregated variables and probably should have selected an data driven approach. Looking first at 
the observed variables and how they influence the dependent variables helps aggregating them into an composite indicator. It also allows for better estimation on which
variables to include. Also the use of structural equation models (SEM) could help finding systems of composite indicators or variables.

It would be a good idea to further investigate how exactly the number of comparison targets influences the answer time. Is there a linear dependence and what is the impact of
each added target.
It would also be interesting to further learn about the effect of the question type as we observed a siginificant difference across types in terms of answer time and accuracy.
Not only which kind of questions are harder to answer but also about what makes
them harder to answer. There exists a whole world of different types of questions that can be answered with the scope of comparing spatial features in a geo-dashboard

\begin{enumerate}
    \item Manipulate one or less variables and try more instances. Some variables were dependent on another. Number of views
    on question type (2 extra views included views on differences) Multicorrinality
    \item Use of MCV was according to the guidelines not that appropriate
    \item Question type opens many opportunities because many possible types are possible when asking for comparison
    \item modify $\beta$ and $\gamma$ from \ref{viewDistractionEquation}
    \item found transformations that result in better fit ask even more for a validation on new data because of a higher 
    chance of overfitting.
    \item If a non-linear transformation significantly improves your model, you might also consider exploring other non-linear models, such as polynomial regression or more complex machine learning models, to see if they provide even better fits.
    \item Since number of comparison targets and question type showed statistical significant differences on log transformed answer time we could further analyse the differences. Which groups showed different answer times and how much?
    One could run Tukey's Honest Significance Difference (HSD) test to find out about the differences between groups. The same research could be done on the answer accuracy and how exactly the accuracy differs across the different question types.

\end{enumerate}