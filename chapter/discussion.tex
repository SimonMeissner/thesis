\chapter{Discussion}
This chapter will first try to interpret the results and relatate the findings to previous work. In the following we will use these results to answer
our research questions. Lastly we will discuss limitations and propose future research directions.
\section{Modelling user performance in mathematical models}
Besides the view quality coefficient the aggregated variables that based on our partly nominal-scaled study variables showed almost no correlation with
the answer time. Trying to fit an linear regression model on the aggregated variables also turned out to be not successful. Because of the exploratory nature
of this study we tried transforming various variables
from the mathematical foundation we assembled. The transformation of the view distraction values showed that an exponential influence of the distraction from each view
better describes the measured answer times in comparison to a linear influence. Including this finding in a multiple linear regression model using all predictors
resulted in a still really poorly descriptive model with an adjusted $R^2$ value of $0.14$. Next up we tried transforming the distraction index from \ref{distractionIndexEquation}
directly. We again found that an exponential influence of the distraction index (which describes the total distraction present) better described our recorded
anser time values compared to an linear influence. The model using again all three variables showed an adjusted $R^2$ value of $0.16$ which meant that the exponentially
transformed distraction index described the answer times slightly better compared to the previous transformation. We tried combining both transformations but the best
multiple linear regression model only reached the same adjusted $R^2$ value of the view distraction transformation. Lastly we aggregated all our aggregated variables
into one called difficulty index (equation \ref{difficultyIndexEquation}) and observed how transformations of this value would perform compared to the other approaches.
The model had and adjusted $R^2$ value of $0.078$ and therefore described the answer times even worse than or initial model with no transformations.
We also fitted multiple linear models over the mean (and in our case also median) values of answer times of every unique variable combination because human performance
tends to vary a lot across different participants. The best model scored an adjusted $R^2$ value of $0.18$ which still indicates a poor descriptive model. Excluding
the number of views variable as it showed no relation to the answer time did only reduce the quality of the model more. Concluding we can say that the distraction index
did not describe our measured answer time values well. It seems that an exponential influence of the distraction performs better compared to an linear influence.
But this must be taken with caution because brute forcing different transformation always come at the cost of fitting the data by chance. It also seems that the number of views
could not describe the answer time very well. Because the inverse term of the quality of views showed an moderate correlation with the answer time and graphically also showed
differences across groups we decided to do an variance analysis which will be interpreted later.

The search for good models describing answer accuracy was similarly sobering. The transformation of the discriminability index that resulted in
the best logristic regression model in terms of p values was the one where the values were squareroot-transformed (p-value of $0.94$). This is 
statistically still an unsignificant description and also the correlation indices showed no correlation with any transformation which in combination is
why we stopped the search for finding mathematical models based on the discriminability index. It seems that the discriminability index could not
accuratly describe the answer accuracy.
\section{User performance depending on interaction technique}
To answer the second research question on which interaction technique best supports the task of comparison in the context of geo-dashboards we chose to analyse the variances
across groups. Regarding answer time we chose to use anova to analyse group differences. The chi squared test method was used to analyse answer accuracy.
We setted the confidence level for all tests to $0.05$. The anova test did not show any significant mean answer time differences across groups as the p-values were above
the confidence level of $0.05$. The chi squared test between the
answer accuracy and the interaction technique showed an p-value way greater than $0.05$ which means that we have to reject the null hypothesis about there beeing significant differences
between different interaction techniques. Regarding the second research question we could not detect any significant user performance differences across the different interaction techniques.
Neither answer time nor answer accuracy seemed to be affected by the interaction technique. Visually small differences between the performance of the interaction techniques regarding answer time
are visible (figure \ref{boxplotsStudyVariablesAnswerTimefigure}), but nothing that could be interpreted as beeing statistically significant as we have seen. Interestingly
the questions about the subjective feedback revealed that the participants disliked \textit{highlighting\_1} more compared to the other techniques.

\section{Additional results}
The variance analysis of the other raw study variables and the answer time delivered mixed results. The anova tests
about the answer time revealed that we can reject the null hypothesis (answer time means are equal across groups) about two of the four variables.
We used levene tests to check for homoscedasticity in all variables. The null hypothesis of the levene tests states that the variances across groups stay equal.
The levene test between the residuals of the anova test between the number of comparison targets and the answer time showed an p-value of $0.029$ displays an significant result
because of the significance level of $0.05$. That means we have to reject the null hypothesis and the condition of homoscedasticity for the anova test is not met.
A log10-transformation of the the answer time resulted in a p-value of $0.66$ in the levene test which confirmed homoscedasticity after the transformation.
The anova test between the log10-transformed answer time values and the number of comparison targets delivered an p-value of $0.00012$ which meant we could reject the
null hypothesis and we have significantly differences between the groups of number of comparison targets regarding the mean log10-transformed answer time values.
Because the number of comparison targets only had two groups (2 targets or 3 targets) we can also conclude that the means of the answer time between comparison
questions addressing 2 comparison targets and comparison questions addressing 3 comparison targets significantly differ. Looking at figure \ref{boxplotsStudyVariablesAnswerTimefigure}
we can graphically derive that the mean of answer time increased with the number of comparison targets. This observation comfirmes the statement of Gleicher that
the comparison task grows more difficult the more targets are compared simultaneously \citep*{Gleicher.2018}. 
The levene test between the residuals of the anova test with untransformed values between the question type and the answer time also delivered an p-value < $0.05$.
We also used a log10-transformation on the answer time to introduce homoscedasticity. The anova test with the question type also showed an p-value of
< $0.05$ ($1.68 \times 10^{-7}$) which means that there are also statistical significant mean differences across the 4 question types in terms of the
log10-transformed answer time. Because we did not include a post-hoc analysis at this point we can only speculate about the types that differ from another. Looking
again at figure \ref{boxplotsStudyVariablesAnswerTimefigure} we can visually see an increase in mean answer time for type4 in comparison to the remaining question types.
This coincides with the often reported high difficulty of the question of type4 using 3 comparison targets from the participants in the subjective feedback.
The anova test on the number of views did not show any significant mean answer time differences across groups as the p-value was way above
the confidence level of $0.05$.

The influence of the study variables on the answer accuracy was measured using the chi squared test and the fisher's exact test. Regarding the
number of view and the number of comparison targets the tests returned an p-value greater than $0.05$ which means
that we have to reject the null hypothesis about there beeing significant differences on the answer accuracy across groups. Interestingly the
chi squared test about the question type returned an p-value of $0.023$ which is < $0.05$. Because one of the precondition of the chi squared test
required the expected cell frequencies of the contengency table to be not smaller than 5 we validated the result using the fisher's exact test which also
returned an p-value < $0.05$ ($0.039$) and therefore indicates significant differences across groups regarding the answer accuracy. Meaning there is a significant
difference between correctly and incorrectly answered questions between at least two groups of question types.

Out of all aggregated variables $\frac{1}{q_{view}}$ was the only variable that showed an moderate positive relation to the answer time with a linear correlation coefficient of $0.33$.
Because the levene test of the residual of the anova test using the untransformed answer time values showed uneven variances across groups the anova test was repeated
using log10-transformed answer values. The levene test returned an p-value of $0.17$ which means the null hypothesis of can be accepted an the variances across groups are even.
The result of the anova test between the log10-transformed answer time values and the $\frac{1}{q_{view}}$ showed that there were siginificant differences between
the two groups of view quality because of the p-value beeing < $0.05$. Together with the correlation coefficient we can conclude that the mean log10-transformed answer times
grow if the quality of the views fall. This does not only give evidence to our assumed mathematical correlation captured in equation \ref{qualityViewEquation} it also backs up
the observations about view quality regarding the information conveyed in the paper from Lohse et al. from 1994 \citep{Lohse.1994}.


\section{Limitations \&\ further research}
The approach of aggregating the variables into continuous variables not only requires careful consideration of the underlying theoretical framework
but also requires previous multivariate analysis to reduce and exclude effects of multicollinearity \citep*{MichelaNardo.2005}. We did not check for multicollinearity
at all.
Trinchera et al. differentiate between three different approaches when trying to formulate aggregated variables \citep*{Trinchera.2008}. We should have
investigated more time in choosing the right approach to formulate our aggregated variables and probably should have selected a data-driven approach. Looking first at 
the observed variables and how they influence the dependent variables helps aggregate them into a composite indicator. It also allows for better estimation of which
variables to include. Also, the use of structural equation models (SEM) could help in finding systems of composite indicators or variables. Next, we want to discuss possible 
reasons for our failure to find good descriptive models.

The missing descriptive power of the number of views could have multiple causes. The first and simplest explanation is that we simply did not collect enough data for it
to show significant differences. The second reason could be that multiple views were not used accordingly. 
It can be argued that the web prototype disregarded some of the proposed guidelines from Baldonado on when to use MCV. The rule of diversity
suggests the existence of complex data with multiple attributes or models \citep*{WangBaldonado.2000}. Our data could be categorized as 1D data as it consisted of 16
time series \citep*{Keim.2005}. It only consisted of one attribute and it was also observed that on the questions that did not ask for differences among
the comparison targets (type1 and type3) the additional views only presented more distraction from the target values. In these questions the additional views did not
help the participant in terms of user performance it only increased cognitive demand. This finding conflicts with our aggregated variable from \ref{difficultyIndexEquation}
where the difficulty index was modeled to be inversely dependent on the number of views. This possible negative effect on the difficulty index and therefore on the
answer time could explain the insignificant differences between answering questions using four views compared to using two views. To better observe the effect of
multiple views future work should focus on using multiple views accordingly which also includes choosing complex enough data that justifies the use.

Because the variance analysis showed the significance of some variables (question type, number of comparison targets)
we can assume that the distraction index did not accurately map the influence of these variables. It could be the case that the impact of different distraction levels regarding
the number of comparison targets or the question type is not an important component of what makes the changing of these variables significant. It could also be that our assumption
for building the difficulty index was not accurate. In this context, it would be interesting to observe the effect of different weights of the similar distractors and the background
distractors ($\beta$ and $\gamma$ of equation \ref{viewDistractionEquation}) on the descriptiveness of the distraction index. It would also be interesting to further learn about the
effect of the question type as we observed a significant difference across types in terms of answer time and accuracy. As explained the distraction index was one try to capture the underlying
differences of comparison questions inside a continuous variable and maybe there are other more important changing factors. The work of Gleicher shows us how many different ideas of comparison 
are imaginable. This opens the possibility to research many more different question types and compare their difficulty \citep*{Gleicher.2018}.

The aggregation of the discriminability index to have a continuous variable that helps us find a mathematical relationship to the answer accuracy
turned out to have no descriptive meaning. The formulation process of a mathematical model describing accuracy was based on fewer scientific assumptions and was also
very vague overall. It could be that the concept of discriminability has no or minor influence on the accuracy. It could also be that we calculated it with wrong
assumptions or failed to collect enough data to see a possible correlation. We would advise to base a variable aggregation on more scientific research or
if not possible follow more of a data-driven approach as discussed to find evidence of what factors could influence such a composite index.
Because the chi-squared tests showed a significant association between the question type and the accuracy we would advise the research of types of comparative questions
and their influence on the accuracy of collecting hints about possible factors that constitute such aggregated variables.

As already indicated one solution to find more significant results could be the use of larger sample sizes. Particularly the analysis of the mean values of the observations is expected to provide more meaningful
results because averaging over two observations per variable combination does not address the issue of high variance of performance across the participants.
The participants were also homogeneous in terms of their age (all 20-29) and experience with geo-dashboards. Our results are therefore not representative
for the general population and all findings have to be regarded to only be applicable to this specific part of the population.

We could not confirm any user performance differences between our different interaction techniques but the asymmetry in the subjective feedback and the slight visual differences
shows the need for additional research. Generally focusing on one or two of our selected variables and adding more groups seems also a good idea in this context. Focusing on
variables that already has been confirmed to show performance differences like the number of comparison targets and then researching the effect of different interaction
techniques should reveal any differences more because the effect of the individual interaction techniques also grows stronger. In this example, the influence of the number
of comparison targets could also be investigated further because more groups than two allow for analysis of underlying mathematical dependency (logarithmic influence, exponential influence, etc.)